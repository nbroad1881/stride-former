# Default configuration

defaults:
  - data: default
  - training_arguments: default
  - model: default
  - mlflow: default
  - wandb: default
  - _self_ # attributes in this file will override defaults


model:
  model_name_or_path:  "google/bigbird-roberta-base"
  short_model_max_chunks: 128
  hidden_act: gelu
  intermediate_size: 3072
  layer_norm_eps: 1e-7
  num_attention_heads: 12
  num_hidden_layers: 24

  
data:
  max_seq_length: 4096
  pad_multiple: 512
  stride: 0
  n_rows: 2000
  map_batch_size: 100 # batch size when using `datasets.map`
  dataset_name: 'ccdv/arxiv-classification'
  
training_arguments:
  do_train: yes
  do_eval: yes
  
  evaluation_strategy: "epoch"
  fp16: yes
  
  learning_rate: 1e-5
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 1
  group_by_length: yes
  
  metric_for_best_model: "f1_micro"
  greater_is_better: yes
  report_to: "wandb"
  log_level: "warning"
  save_strategy: "epoch"
  logging_steps: 10
  save_total_limit: 2
  
# general configuration
num_proc: 2
task: text-classification
language: en
project_name: stride-former

hydra:
  output_subdir: config